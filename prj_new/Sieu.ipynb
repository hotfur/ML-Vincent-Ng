{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-NJ2QoENDES"
   },
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nedc4PEzMxZs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os, sys\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.preprocessing import *\n",
    "from multiprocessing import Pool\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import *\n",
    "\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.naive_bayes import *\n",
    "\n",
    "os.chdir(\"/home/sieu/PycharmProjects/ML-Vincent-Ng/prj_new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lqb0XC5HOl5q"
   },
   "source": [
    "# Training import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MqbBwV2BkPtY"
   },
   "outputs": [],
   "source": [
    "column_name = pd.read_table('attr.txt', sep=\":\", usecols=all, names = ['attr', 'range'])\n",
    "df = pd.read_table('train.txt', sep=\"\\s+\", usecols=all, names = list(column_name['attr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Y2TvwGLmvS_e"
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(90, 90))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate class from attributes\n",
    "y = df[\"Class\"].to_numpy()\n",
    "X = df[df.columns[:-1]]\n",
    "attr_lst = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B1' 'B2' 'B3' 'C1' 'C2' 'C3' 'C4' 'C5' 'C6' 'C7' 'C8' 'C9' 'C10' 'C11'\n",
      " 'C12' 'C13' 'C14' 'C15' 'C16' 'C17' 'C18' 'C19' 'C20' 'C21' 'C22' 'C23'\n",
      " 'C24' 'C25' 'C26' 'C27' 'C28' 'C29' 'C30' 'C31' 'C32' 'C33' 'C34' 'C35'\n",
      " 'C36' 'C37' 'C38' 'C39' 'C40' 'C41' 'C42' 'C43' 'C44' 'C45' 'C46' 'C47'\n",
      " 'C48' 'C49' 'C50' 'C51' 'C52' 'C53' 'C54' 'C55' 'C56' 'C57' 'C58' 'C59'\n",
      " 'C60' 'C61' 'C62' 'C63' 'C64' 'C65' 'C66' 'C67' 'C68' 'C69' 'C70' 'C71'\n",
      " 'C72' 'C73' 'C74' 'C75' 'C76' 'C77' 'C78' 'C79' 'C80' 'C81' 'C82' 'C83'\n",
      " 'C84' 'C85' 'C86' 'C87' 'C88' 'C89' 'C90' 'C91' 'C92' 'C93' 'C94' 'C95'\n",
      " 'C96' 'C97' 'C98' 'C99' 'C100' 'C101' 'C102' 'C103' 'C104' 'C105' 'C106'\n",
      " 'C107' 'C108' 'C109' 'C110' 'C111' 'C112' 'C113' 'C114' 'C115' 'C116'\n",
      " 'C117' 'C118' 'C119' 'C120' 'C121' 'C122' 'C123' 'C124' 'C125' 'C126'\n",
      " 'C127' 'C128' 'C129' 'C130' 'C131' 'C132' 'C133' 'C134' 'C135' 'C136'\n",
      " 'C137' 'C138' 'C139' 'YEAR' 'C140' 'C141' 'C142' 'CT1' 'CT2' 'CT3' 'CT4'\n",
      " 'CT5' 'CT6' 'CT9' 'CT10' 'CT11' 'CT12' 'CT13' 'CT14' 'CT15' 'CT16' 'CT17'\n",
      " 'CT18' 'CT19' 'CT20' 'CT21' 'CT22' 'CT23' 'CT24' 'CT25' 'CT26' 'CH1'\n",
      " 'CH3']\n",
      "Number of features:  172\n",
      "(11982, 172)\n"
     ]
    }
   ],
   "source": [
    "# Remove all constant-valued features\n",
    "sel = VarianceThreshold()\n",
    "sel.feature_names_in_= attr_lst\n",
    "X=sel.fit_transform(X)\n",
    "attr_lst=sel.get_feature_names_out(attr_lst)\n",
    "print(attr_lst)\n",
    "print(\"Number of features: \", len(attr_lst))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select good features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN: BernoulliNB classifier feature selection\n",
      "['B1' 'B2' 'C1' 'C2' 'C3' 'C4' 'C6' 'C7' 'C8' 'C10' 'C11' 'C15' 'C23'\n",
      " 'C26' 'C28' 'C31' 'C41' 'C46' 'C56' 'C71' 'C76' 'C86' 'C101' 'C116'\n",
      " 'C131' 'C136' 'C137' 'C138' 'C139' 'YEAR' 'C140' 'C141' 'C142' 'CT1'\n",
      " 'CT2' 'CT3' 'CT4' 'CT5' 'CT6' 'CT9' 'CT10' 'CT11' 'CT12' 'CT13' 'CT14'\n",
      " 'CT15' 'CT16' 'CT17' 'CT18' 'CT19' 'CT20' 'CT21' 'CT22' 'CT23' 'CT24'\n",
      " 'CT25' 'CT26']\n",
      "Done BernoulliNB classifier feature selection\n"
     ]
    }
   ],
   "source": [
    "n_fet_to_sel = len(attr_lst)//3\n",
    "\n",
    "# print(\"BEGIN: BernoulliNB classifier feature selection\")\n",
    "# berNB = SequentialFeatureSelector(estimator=BernoulliNB(),n_features_to_select=n_fet_to_sel, direction=\"forward\").fit(X, y)\n",
    "# berNB.feature_names_in_= attr_lst\n",
    "# attr_lst = berNB.get_feature_names_out(attr_lst)\n",
    "# print(attr_lst)\n",
    "# print(\"Done BernoulliNB classifier feature selection\")\n",
    "\n",
    "print(\"BEGIN: HistGrad classifier feature selection\")\n",
    "berNB = SequentialFeatureSelector(estimator=HistGradientBoostingClassifier(max_leaf_nodes=60,max_iter=3000,learning_rate=0.06,l2_regularization=0.15, max_depth=8, max_bins=24, early_stopping=True, random_state=0),n_features_to_select=n_fet_to_sel, direction=\"forward\").fit(X, y)\n",
    "berNB.feature_names_in_= attr_lst\n",
    "attr_lst = berNB.get_feature_names_out(attr_lst)\n",
    "print(attr_lst)\n",
    "print(\"Done HistGrad classifier feature selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature selection results\n",
    "with open(\"bernolli_fet_1,3.pkl\", 'wb') as file:\n",
    "    pickle.dump(attr_lst, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load good feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B1' 'B2' 'C1' 'C2' 'C3' 'C4' 'C6' 'C7' 'C8' 'C10' 'C11' 'C15' 'C23'\n",
      " 'C26' 'C28' 'C31' 'C41' 'C46' 'C56' 'C71' 'C76' 'C86' 'C101' 'C116'\n",
      " 'C131' 'C136' 'C137' 'C138' 'C139' 'YEAR' 'C140' 'C141' 'C142' 'CT1'\n",
      " 'CT2' 'CT3' 'CT4' 'CT5' 'CT6' 'CT9' 'CT10' 'CT11' 'CT12' 'CT13' 'CT14'\n",
      " 'CT15' 'CT16' 'CT17' 'CT18' 'CT19' 'CT20' 'CT21' 'CT22' 'CT23' 'CT24'\n",
      " 'CT25' 'CT26']\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "# Load good feature list\n",
    "with open(\"bernolli_fet_1,3.pkl\", 'rb') as file:\n",
    "    attr_lst = pickle.load(file)\n",
    "print(attr_lst)\n",
    "print(len(attr_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from clipboard\n",
    "attr_lst = ['B1', 'B3', 'C1', 'C2', 'C3', 'C4', 'C6', 'C7', 'C8', 'C12', 'C14',\n",
    "       'C23', 'C32', 'C39', 'C40', 'C41', 'C45', 'C46', 'C51', 'C54',\n",
    "       'C77', 'C91', 'C92', 'C99', 'C101', 'C104', 'C105', 'C111', 'C116',\n",
    "       'C137', 'C141', 'CT18', 'CT19', 'CT22']\n",
    "attr_lst=np.array(attr_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7597998331943286\n",
      "0.7297748123436196\n",
      "0.7562604340567612\n",
      "0.7278797996661102\n",
      "0.7454090150250418\n",
      "0.7353923205342237\n",
      "0.7487479131886478\n",
      "0.7454090150250418\n",
      "0.7545909849749582\n",
      "0.7504173622704507\n",
      "Done\n",
      "mean acc:  0.7453681490279183\n"
     ]
    }
   ],
   "source": [
    "# Best model validation\n",
    "X_new=df[attr_lst].to_numpy()\n",
    "kfold = sklearn.model_selection.KFold(n_splits=10, shuffle=True)\n",
    "arr = []\n",
    "#clf = BernoulliNB()\n",
    "clf = HistGradientBoostingClassifier(max_leaf_nodes=60,max_iter=3000,learning_rate=0.06,l2_regularization=0.15, max_depth=8, max_bins=24, early_stopping=True)\n",
    "for train_index , test_index in kfold.split(X_new):\n",
    "    X_train, X_test = X_new[train_index], X_new[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(clf.score(X_test, y_test))\n",
    "    arr.append(clf.score(X_test, y_test))\n",
    "print(\"Done\")\n",
    "print(\"mean acc: \", np.mean(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B1', 'B2', 'C1', 'C2', 'C3', 'C4', 'C6', 'C7', 'C8', 'C10', 'C11',\n",
       "       'C15', 'C23', 'C26', 'C28', 'C31', 'C41', 'C46', 'C56', 'C71',\n",
       "       'C76', 'C86', 'C101', 'C116', 'C131', 'C136', 'C137', 'C138',\n",
       "       'C139', 'YEAR', 'C140', 'C141', 'C142', 'CT1', 'CT2', 'CT3', 'CT4',\n",
       "       'CT5', 'CT6', 'CT9', 'CT10', 'CT11', 'CT12', 'CT13', 'CT14',\n",
       "       'CT15', 'CT16', 'CT17', 'CT18', 'CT19', 'CT20', 'CT21', 'CT22',\n",
       "       'CT23', 'CT24', 'CT25', 'CT26'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch for best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive PCA\n",
    "params = {'max_leaf_nodes':np.arange(200,500, 20), 'l2_regularization':np.arange(0.5,1,0.05), 'learning_rate':np.arange(0.3,1,0.05), 'max_depth':np.arange(8, 40, 2), 'max_bins':np.arange(20, 50, 2)}\n",
    "#params = {'max_iter':np.arange(50,200,100),'max_leaf_nodes':np.arange(100,500, 10), 'l2_regularization':np.arange(0,1,0.05)}\n",
    "search = GridSearchCV(estimator=HistGradientBoostingClassifier(categorical_features=[0, 1, 2]), \n",
    "                                param_grid=params).fit(X_new,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Grid\n",
    "params = {'max_leaf_nodes':np.arange(20,90, 10), 'l2_regularization':np.arange(0.05,0.4,0.05), 'learning_rate':np.arange(0.01,0.1,0.01), 'max_depth':np.arange(6, 12, 1), 'max_bins':np.arange(16, 34, 2)}\n",
    "#params = {'max_iter':np.arange(50,200,100),'max_leaf_nodes':np.arange(100,500, 10), 'l2_regularization':np.arange(0,1,0.05)}\n",
    "search = GridSearchCV(estimator=HistGradientBoostingClassifier(max_iter=3000, early_stopping=True), \n",
    "                                param_grid=params, n_jobs=-1).fit(X_new,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8158905024202971"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.score(X_new, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "with open(\"815_1,3_best.pkl\", 'wb') as file:\n",
    "    pickle.dump(search, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "with open(\"80_best.pkl\", 'rb') as file:\n",
    "    search = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.score(X_new, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l2_regularization': 0.15000000000000002,\n",
       " 'learning_rate': 0.060000000000000005,\n",
       " 'max_bins': 24,\n",
       " 'max_depth': 8,\n",
       " 'max_leaf_nodes': 60}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model validation\n",
    "kfold = sklearn.model_selection.KFold(n_splits=3, shuffle=True)\n",
    "arr = []\n",
    "#clf = HistGradientBoostingClassifier(categorical_features=[0, 1, 2, 4, 5, 6], max_leaf_nodes=50,max_iter=3000,learning_rate=0.06,l2_regularization=0.25, max_depth=11, max_bins=42, early_stopp)\n",
    "for train_index , test_index in kfold.split(X_new):\n",
    "    X_train, X_test = X_new[train_index], X_new[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    #clf.fit(X_train, y_train)\n",
    "    #print(clf.score(X_test, y_test))\n",
    "    #arr.append(clf.score(X_test, y_test))\n",
    "    arr.append(search.score(X_test, y_test))\n",
    "    print(search.score(X_test, y_test))\n",
    "print(\"Done\")\n",
    "print(\"mean acc: \", np.mean(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ridge_classifiers(data):\n",
    "    clf = RidgeClassifier(solver=\"svd\")\n",
    "    clf.fit(data[0], data[1])\n",
    "    score = clf.score(data[2], data[3])\n",
    "    return (clf, score)\n",
    "def knn_classifiers(data):\n",
    "    clf = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "    clf.fit(data[0], data[1])\n",
    "    score = clf.score(data[2], data[3])\n",
    "    return (clf, score)\n",
    "def svc_classifiers(data):\n",
    "    clf = SVC(kernel='rbf', degree=3)\n",
    "    clf.fit(data[0], data[1])\n",
    "    score = clf.score(data[2], data[3])\n",
    "    return (clf, score)\n",
    "def BernoulliNB_classifiers(data):\n",
    "    clf = BernoulliNB()\n",
    "    clf.fit(data[0], data[1])\n",
    "    score = clf.score(data[2], data[3])\n",
    "    return (clf, score)\n",
    "def HGB_classifiers(data):\n",
    "    clf = HistGradientBoostingClassifier(categorical_features=[0, 1, 2])\n",
    "    clf.fit(data[0], data[1])\n",
    "    score = clf.score(data[2], data[3])\n",
    "    return (clf, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = sklearn.model_selection.KFold(n_splits=10, shuffle=True)\n",
    "dataset = []\n",
    "for train_index , test_index in kfold.split(X):\n",
    "    X_train, X_test = X_new[train_index], X_new[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    dataset.append((X_train, y_train, X_test, y_test))\n",
    "    #Testing individual clf\n",
    "    clf = HistGradientBoostingClassifier(categorical_features=[0, 1, 2], max_leaf_nodes=800,max_iter=300,learning_rate=0.8500000000000002,l2_regularization=0.8500000000000001).fit(X_train, y_train)\n",
    "    #assembly.append(clf)\n",
    "    #vote_weights.append(clf.score(X_test, y_test))\n",
    "    #clf = RidgeClassifier(solver=\"svd\").fit(X_train, y_train)\n",
    "    #assembly.append(clf)\n",
    "    #vote_weights.append(clf.score(X_test, y_test))\n",
    "    #clf = BernoulliNB().fit(X_train, y_train)\n",
    "    #assembly.append(clf)\n",
    "    #vote_weights.append(clf.score(X_test, y_test))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly = []\n",
    "vote_weights = []\n",
    "def append_results(result):\n",
    "    for clf, score in result:\n",
    "        assembly.append(clf)\n",
    "        vote_weights.append(score)\n",
    "if __name__ == '__main__':\n",
    "    p = Pool()\n",
    "    p.map_async(Ridge_classifiers, dataset, callback=append_results)\n",
    "    #p.map_async(nn_classifiers, dataset, callback=append_results)\n",
    "    #p.map_async(knn_classifiers, dataset, callback=append_results)\n",
    "    #p.map_async(svc_classifiers, dataset, callback=append_results)\n",
    "    p.map_async(BernoulliNB_classifiers, dataset, callback=append_results)\n",
    "    p.map_async(HGB_classifiers, dataset, callback=append_results)\n",
    "    p.close()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assembly)\n",
    "print(vote_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "for i in range(len(assembly)):\n",
    "    clfs.append((\"Classifier \" + str(i), assembly[i]))\n",
    "eclf = VotingClassifier(estimators=clfs,voting='hard', weights=vote_weights, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(eclf, X_new, y, scoring='accuracy', cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGTWpkXcZ7MH"
   },
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = pd.read_table('attr.txt', sep=\":\", usecols=all, names = ['attr', 'range'])\n",
    "dfX_pred = pd.read_table('prelim.txt', sep=\"\\s+\", usecols=all, names = list(attr['attr']))\n",
    "dfX_pred = dfX_pred.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C3',\n",
       " 'C4',\n",
       " 'C6',\n",
       " 'C7',\n",
       " 'C8',\n",
       " 'C10',\n",
       " 'C11',\n",
       " 'C15',\n",
       " 'C23',\n",
       " 'C26',\n",
       " 'C28',\n",
       " 'C31',\n",
       " 'C41',\n",
       " 'C46',\n",
       " 'C56',\n",
       " 'C71',\n",
       " 'C76',\n",
       " 'C86',\n",
       " 'C101',\n",
       " 'C116',\n",
       " 'C131',\n",
       " 'C136',\n",
       " 'C137',\n",
       " 'C138',\n",
       " 'C139',\n",
       " 'YEAR',\n",
       " 'C140',\n",
       " 'C141',\n",
       " 'C142',\n",
       " 'CT1',\n",
       " 'CT2',\n",
       " 'CT3',\n",
       " 'CT4',\n",
       " 'CT5',\n",
       " 'CT6',\n",
       " 'CT9',\n",
       " 'CT10',\n",
       " 'CT11',\n",
       " 'CT12',\n",
       " 'CT13',\n",
       " 'CT14',\n",
       " 'CT15',\n",
       " 'CT16',\n",
       " 'CT17',\n",
       " 'CT18',\n",
       " 'CT19',\n",
       " 'CT20',\n",
       " 'CT21',\n",
       " 'CT22',\n",
       " 'CT23',\n",
       " 'CT24',\n",
       " 'CT25',\n",
       " 'CT26']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"bernolli_fet_1,3.pkl\", 'rb') as file:\n",
    "    attr_lst = pickle.load(file)\n",
    "list(attr_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from clipboard\n",
    "attr_lst = ['B1', 'B2', 'C1', 'C2', 'C3', 'C4', 'C6', 'C7', 'C8', 'C10', 'C11',\n",
    "       'C15', 'C23', 'C26', 'C28', 'C31', 'C41', 'C46', 'C56', 'C71',\n",
    "       'C76', 'C86', 'C101', 'C116', 'C131', 'C136', 'C137', 'C138',\n",
    "       'C139', 'YEAR', 'C140', 'C141', 'C142', 'CT1', 'CT2', 'CT3', 'CT4',\n",
    "       'CT5', 'CT6', 'CT9', 'CT10', 'CT11', 'CT12', 'CT13', 'CT14',\n",
    "       'CT15', 'CT16', 'CT17', 'CT18', 'CT19', 'CT20', 'CT21', 'CT22',\n",
    "       'CT23', 'CT24', 'CT25', 'CT26']\n",
    "attr_lst=np.array(attr_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = dfX_pred[attr_lst].to_numpy()\n",
    "y_pred = dfX_pred[\"Class\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation score is:  0.7434\n"
     ]
    }
   ],
   "source": [
    "# Load model then predict and save prediction\n",
    "with open(\"815_1,3_best.pkl\", 'rb') as file:\n",
    "    search = pickle.load(file)\n",
    "print('The validation score is: ', search.score(X_pred, y_pred))\n",
    "np.savetxt(fname=\"prediction.txt\", X=search.predict(X_pred), fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
