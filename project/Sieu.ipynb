{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-NJ2QoENDES"
   },
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nedc4PEzMxZs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os, sys\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import *\n",
    "\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "os.chdir(\"/home/sieu/PycharmProjects/ML-Vincent-Ng/project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lqb0XC5HOl5q"
   },
   "source": [
    "# Training import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MqbBwV2BkPtY"
   },
   "outputs": [],
   "source": [
    "column_name = pd.read_table('attr.txt', sep=\":\", usecols=all, names = ['attr', 'range'])\n",
    "df = pd.read_table('train.txt', sep=\"\\s+\", usecols=all, names = list(column_name['attr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQmTIelVmexu",
    "outputId": "f747088a-64d9-4766-bb0a-9e96f84a4221"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>...</th>\n",
       "      <th>CT22</th>\n",
       "      <th>CT23</th>\n",
       "      <th>CT24</th>\n",
       "      <th>CT25</th>\n",
       "      <th>CT26</th>\n",
       "      <th>CH1</th>\n",
       "      <th>CH2</th>\n",
       "      <th>CH3</th>\n",
       "      <th>CH4</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.260</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.720</td>\n",
       "      <td>4.590</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.540</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.540</td>\n",
       "      <td>2.88</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.810</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.720</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.900</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.810</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.450</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.630</td>\n",
       "      <td>1.215</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.540</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.085</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.945</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.170</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.810</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.810</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11982 rows Ã— 176 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       B1  B2  B3     C1    C2     C3     C4    C5     C6     C7  ...  CT22  \\\n",
       "0       0   0   1  1.260  1.17  0.720  4.590  0.45  0.765  0.540  ...     3   \n",
       "1       0   1   0  0.450  0.81  0.000  0.000  0.00  0.855  0.000  ...     3   \n",
       "2       0   0   1  0.540  2.88  0.000  0.000  0.00  0.765  0.000  ...     3   \n",
       "3       0   0   1  0.810  1.35  0.450  0.000  0.00  0.000  0.720  ...     3   \n",
       "4       0   0   1  0.900  1.17  0.765  0.000  0.00  0.630  0.810  ...     4   \n",
       "...    ..  ..  ..    ...   ...    ...    ...   ...    ...    ...  ...   ...   \n",
       "11995   0   1   0  0.450  1.17  0.630  1.215  0.45  0.990  0.540  ...     3   \n",
       "11996   0   1   0  5.085  0.90  0.000  0.000  0.00  0.000  0.945  ...     4   \n",
       "11997   0   0   1  1.170  1.71  0.630  0.000  0.00  0.720  0.000  ...     3   \n",
       "11998   0   0   1  0.810  1.17  0.000  0.000  0.00  0.990  0.810  ...     5   \n",
       "11999   0   0   1  0.000  0.90  0.720  0.000  0.00  0.000  0.000  ...     5   \n",
       "\n",
       "       CT23  CT24  CT25  CT26  CH1  CH2  CH3  CH4  Class  \n",
       "0         2     2     1     3    0    0    0    0      1  \n",
       "1         2     1     1     3    0    0    0    0      1  \n",
       "2         2     2     1     3    0    0    0    0      1  \n",
       "3         2     2     1     3    0    0    0    0      1  \n",
       "4         3     2     1     3    0    0    0    0      1  \n",
       "...     ...   ...   ...   ...  ...  ...  ...  ...    ...  \n",
       "11995     2     2     1     3    0    0    0    0      0  \n",
       "11996     3     2     1     3    0    0    0    0      1  \n",
       "11997     2     1     1     3    0    0    0    0      0  \n",
       "11998     3     2     2     4    0    0    0    0      1  \n",
       "11999     3     2     1     3    0    0    0    0      0  \n",
       "\n",
       "[11982 rows x 176 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "df = df.dropna()\n",
    "# Save year\n",
    "X_year =  df.loc[:,'YEAR']\n",
    "df.drop(['YEAR'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Y2TvwGLmvS_e"
   },
   "outputs": [],
   "source": [
    "# Seperate data by type\n",
    "y = df[\"Class\"].to_numpy()\n",
    "X_B = df.loc[:, 'B1':'B3']\n",
    "X_C = df.loc[:,'C1':'C139']\n",
    "X_CT = df.loc[:,'CT1':'CT26']\n",
    "X_H = df.loc[:,'CH1':'CH4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "print(X_B.describe())\n",
    "print(X_C.describe())\n",
    "print(X_CT.describe())\n",
    "print(X_H.describe())\n",
    "print(X_year.describe())\n",
    "print(np.unique(df[[\"CT22\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select good features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are the columns retained by each\n",
    "# List contains all attributes retained during feature selection: attr_lst\n",
    "x_c_cols = X_C.columns\n",
    "x_ct_cols = X_CT.columns\n",
    "x_h_cols = X_H.columns\n",
    "# Feature selection based on variance threshold for every type of data\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "\n",
    "# X_C dataframe\n",
    "sel.feature_names_in_= x_c_cols\n",
    "X_C = sel.fit_transform(X_C)\n",
    "x_c_cols=sel.get_feature_names_out(x_c_cols)\n",
    "attr_lst=x_c_cols\n",
    "print(x_c_cols)\n",
    "print(\"Number of C* features: \", len(x_c_cols))\n",
    "\n",
    "# X_CT dataframe\n",
    "sel.feature_names_in_= x_ct_cols\n",
    "X_CT = sel.fit_transform(X_CT)\n",
    "x_ct_cols=sel.get_feature_names_out(x_ct_cols)\n",
    "attr_lst=np.concatenate((attr_lst, x_ct_cols))\n",
    "print(x_ct_cols)\n",
    "print(\"Number of CT* features: \", len(x_ct_cols))\n",
    "\n",
    "# X_H dataframe\n",
    "sel.feature_names_in_= x_h_cols\n",
    "X_H = sel.fit_transform(X_H)\n",
    "x_h_cols=sel.get_feature_names_out(x_h_cols)\n",
    "attr_lst=np.concatenate((attr_lst, x_h_cols))\n",
    "print(x_h_cols)\n",
    "print(\"Number of CH* features: \", len(x_h_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "selection_factor = 5\n",
    "attr_lst = []\n",
    "\n",
    "if len(x_c_cols) > 0:    \n",
    "    print(\"BEGIN: Ridge classifier feature selection for C* atttributes\")\n",
    "    sel_X_C = SequentialFeatureSelector(estimator=RidgeClassifierCV(), n_features_to_select=max(len(x_c_cols)//(selection_factor), 1), direction=\"forward\")\n",
    "    sel_X_C.feature_names_in_= x_c_cols\n",
    "    X_C = sel_X_C.fit_transform(X_C, y)\n",
    "    x_c_cols = sel_X_C.get_feature_names_out(x_c_cols)\n",
    "    attr_lst=np.concatenate((attr_lst, x_c_cols))\n",
    "    print(x_c_cols)\n",
    "    print(\"Done Ridge classifier feature selection for C* atttributes\")\n",
    "\n",
    "if len(x_ct_cols) > 0:\n",
    "    print(\"BEGIN: CategoricalNB classifier feature selection for CT* atttributes\")\n",
    "    sel_X_CT = SequentialFeatureSelector(estimator=CategoricalNB(), n_features_to_select=max(len(x_ct_cols)//selection_factor, 1), direction=\"forward\")\n",
    "    sel_X_CT.feature_names_in_= x_ct_cols\n",
    "    X_CT = sel_X_CT.fit_transform(X_CT, y)\n",
    "    x_ct_cols = sel_X_CT.get_feature_names_out(x_ct_cols)\n",
    "    attr_lst=np.concatenate((attr_lst, x_ct_cols))\n",
    "    print(x_ct_cols)\n",
    "    print(\"Done CategoricalNB classifier feature selection for CT* atttributes\")\n",
    "\n",
    "if len(x_h_cols) > 0:\n",
    "    print(\"BEGIN: DecisionTreeClassifier feature selection for CH* atttributes\")\n",
    "    sel_X_H= SequentialFeatureSelector(estimator=DecisionTreeClassifier(), n_features_to_select=max(len(x_h_cols)//selection_factor, 1), direction=\"forward\")\n",
    "    sel_X_H.feature_names_in_= x_h_cols\n",
    "    X_H = sel_X_H.fit_transform(X_H, y)\n",
    "    x_h_cols = sel_X_H.get_feature_names_out(x_h_cols)\n",
    "    attr_lst=np.concatenate((attr_lst, x_h_cols))\n",
    "    print(x_h_cols)\n",
    "    print(\"Done DecisionTreeClassifier feature selection for CH* atttributes\")\n",
    "\n",
    "X=np.concatenate((X_C, X_CT, X_H), axis=1)\n",
    "print(attr_lst)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature selection results\n",
    "with open(\"_fet.pkl\", 'wb') as file:\n",
    "    pickle.dump(attr_lst, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load good feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C1' 'C3' 'C5' 'C6' 'C8' 'C13' 'C16' 'C17' 'C19' 'C26' 'C31' 'C48' 'C53'\n",
      " 'C59' 'C64' 'C69' 'C70' 'C94' 'C97' 'CT3' 'CT16' 'CT26' 'CH3']\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "# Load good feature list\n",
    "with open(\"25_fet.pkl\", 'rb') as file:\n",
    "    attr_lst = pickle.load(file)\n",
    "print(attr_lst)\n",
    "print(len(attr_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from clipboard\n",
    "attr_lst = ['C1', 'C3', 'C5', 'C6', 'C8', 'C13', 'C16', 'C17', 'C19', 'C26', 'C31', 'C48', 'C53', 'C59', 'C64', 'C69', 'C70', 'C94', 'C97', 'CT3', 'CT16', 'CT26', 'CH3']\n",
    "attr_lst=np.array(attr_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C1' 'C3' 'C5' 'C6' 'C8' 'C13' 'C16' 'C17' 'C19' 'C26' 'C31' 'C48' 'C53'\n",
      " 'C59' 'C64' 'C69' 'C70' 'C94' 'C97' 'CT3' 'CT16' 'CT26' 'CH3']\n"
     ]
    }
   ],
   "source": [
    "attr_lst=attr_lst.astype(str)\n",
    "print(attr_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11982, 25)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive PCA\n",
    "X = df[attr_lst].to_numpy()\n",
    "pca = PCA(svd_solver='full', n_components='mle')\n",
    "X = pca.fit_transform(X)\n",
    "X_new=np.concatenate((X_B, X), axis=1)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C1' 'C3' 'C5' 'C6' 'C8' 'C13' 'C16' 'C17' 'C19' 'C26' 'C31' 'C48' 'C53'\n",
      " 'C59' 'C64' 'C69' 'C70' 'C94' 'C97']\n",
      "Perform PCA on continuous features to filter noises\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "# Reseperate C data to perform PCA\n",
    "non_C_ind = np.logical_or((np.core.defchararray.find(attr_lst,\"CT\")!=-1),(np.core.defchararray.find(attr_lst,\"CH\")!=-1))\n",
    "C_label = attr_lst[np.logical_not(non_C_ind)]\n",
    "attr_lst = attr_lst[non_C_ind]\n",
    "print(C_label)\n",
    "X = df[C_label].to_numpy()\n",
    "print('Perform PCA on continuous features to filter noises')\n",
    "pca = PCA(svd_solver='full', n_components='mle')\n",
    "X = pca.fit_transform(X)\n",
    "no_component = len(pca.get_feature_names_out())\n",
    "print(no_component)\n",
    "# Concat X_B with X to get the final attribute array\n",
    "X_new=np.concatenate((X_B, df[attr_lst], X), axis=1)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11982, 25)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch for best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive PCA\n",
    "params = {'max_leaf_nodes':np.arange(200,500, 20), 'l2_regularization':np.arange(0.5,1,0.05), 'learning_rate':np.arange(0.3,1,0.05), 'max_depth':np.arange(8, 40, 2), 'max_bins':np.arange(70, 150, 2)}\n",
    "#params = {'max_iter':np.arange(50,200,100),'max_leaf_nodes':np.arange(100,500, 10), 'l2_regularization':np.arange(0,1,0.05)}\n",
    "search = GridSearchCV(estimator=HistGradientBoostingClassifier(categorical_features=[0, 1, 2]), \n",
    "                                param_grid=params).fit(X_new,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Grid\n",
    "params = {'max_leaf_nodes':np.arange(100,250, 10), 'l2_regularization':np.arange(0.5,1,0.05), 'learning_rate':np.arange(0.05,0.3,0.05), 'max_depth':np.arange(6, 18, 1), 'max_bins':np.arange(20, 70, 2)}\n",
    "#params = {'max_iter':np.arange(50,200,100),'max_leaf_nodes':np.arange(100,500, 10), 'l2_regularization':np.arange(0,1,0.05)}\n",
    "search = GridSearchCV(estimator=HistGradientBoostingClassifier(max_iter=1000, categorical_features=np.arange(0,X_new.shape[1]-no_component), early_stopping=True), \n",
    "                                param_grid=params, n_jobs=-1).fit(X_new,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8982485404503753\n",
      "0.8840700583819849\n",
      "0.8898163606010017\n",
      "0.9048414023372288\n",
      "0.8973288814691152\n",
      "0.8898163606010017\n",
      "0.9106844741235393\n",
      "0.9098497495826378\n",
      "0.9090150250417363\n",
      "0.9031719532554258\n",
      "Done\n",
      "mean acc:  0.8996842805844046\n"
     ]
    }
   ],
   "source": [
    "# Best model validation\n",
    "kfold = sklearn.model_selection.KFold(n_splits=10, shuffle=True)\n",
    "arr = []\n",
    "for train_index , test_index in kfold.split(X):\n",
    "    X_train, X_test = X_new[train_index], X_new[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    #clf = HistGradientBoostingClassifier(categorical_features=[0, 1, 2, 4,5,6], max_leaf_nodes=300,max_iter=300,learning_rate=0.5,l2_regularization=0.95).fit(X_train, y_train)\n",
    "    #print(clf.score(X_test, y_test))\n",
    "    #arr.append(clf.score(X_test, y_test))\n",
    "    arr.append(search.score(X_test, y_test))\n",
    "    print(search.score(X_test, y_test))\n",
    "print(\"Done\")\n",
    "print(\"mean acc: \", np.mean(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "with open(\"927_newmethod.pkl\", 'wb') as file:\n",
    "    pickle.dump(search, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "with open(\"888_low_depth_leaf_model_newmethod.pkl\", 'rb') as file:\n",
    "    search = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_leaf_nodes': 200,\n",
       " 'max_depth': 36,\n",
       " 'max_bins': 118,\n",
       " 'learning_rate': 0.3,\n",
       " 'l2_regularization': 0.55}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ridge_classifiers(data):\n",
    "    clf = RidgeClassifier(solver=\"svd\")\n",
    "    clf.fit(data[0], data[1])\n",
    "    score = clf.score(data[2], data[3])\n",
    "    return (clf, score)\n",
    "def knn_classifiers(data):\n",
    "    clf = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "    clf.fit(data[0], data[1])\n",
    "    score = clf.score(data[2], data[3])\n",
    "    return (clf, score)\n",
    "def svc_classifiers(data):\n",
    "    clf = SVC(kernel='rbf', degree=3)\n",
    "    clf.fit(data[0], data[1])\n",
    "    score = clf.score(data[2], data[3])\n",
    "    return (clf, score)\n",
    "def BernoulliNB_classifiers(data):\n",
    "    clf = BernoulliNB()\n",
    "    clf.fit(data[0], data[1])\n",
    "    score = clf.score(data[2], data[3])\n",
    "    return (clf, score)\n",
    "def HGB_classifiers(data):\n",
    "    clf = HistGradientBoostingClassifier(categorical_features=[0, 1, 2])\n",
    "    clf.fit(data[0], data[1])\n",
    "    score = clf.score(data[2], data[3])\n",
    "    return (clf, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = sklearn.model_selection.KFold(n_splits=10, shuffle=True)\n",
    "dataset = []\n",
    "for train_index , test_index in kfold.split(X):\n",
    "    X_train, X_test = X_new[train_index], X_new[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    dataset.append((X_train, y_train, X_test, y_test))\n",
    "    #Testing individual clf\n",
    "    clf = HistGradientBoostingClassifier(categorical_features=[0, 1, 2], max_leaf_nodes=800,max_iter=300,learning_rate=0.8500000000000002,l2_regularization=0.8500000000000001).fit(X_train, y_train)\n",
    "    #assembly.append(clf)\n",
    "    #vote_weights.append(clf.score(X_test, y_test))\n",
    "    #clf = RidgeClassifier(solver=\"svd\").fit(X_train, y_train)\n",
    "    #assembly.append(clf)\n",
    "    #vote_weights.append(clf.score(X_test, y_test))\n",
    "    #clf = BernoulliNB().fit(X_train, y_train)\n",
    "    #assembly.append(clf)\n",
    "    #vote_weights.append(clf.score(X_test, y_test))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly = []\n",
    "vote_weights = []\n",
    "def append_results(result):\n",
    "    for clf, score in result:\n",
    "        assembly.append(clf)\n",
    "        vote_weights.append(score)\n",
    "if __name__ == '__main__':\n",
    "    p = Pool()\n",
    "    p.map_async(Ridge_classifiers, dataset, callback=append_results)\n",
    "    #p.map_async(nn_classifiers, dataset, callback=append_results)\n",
    "    #p.map_async(knn_classifiers, dataset, callback=append_results)\n",
    "    #p.map_async(svc_classifiers, dataset, callback=append_results)\n",
    "    p.map_async(BernoulliNB_classifiers, dataset, callback=append_results)\n",
    "    p.map_async(HGB_classifiers, dataset, callback=append_results)\n",
    "    p.close()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assembly)\n",
    "print(vote_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "for i in range(len(assembly)):\n",
    "    clfs.append((\"Classifier \" + str(i), assembly[i]))\n",
    "eclf = VotingClassifier(estimators=clfs,voting='hard', weights=vote_weights, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(eclf, X_new, y, scoring='accuracy', cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGTWpkXcZ7MH"
   },
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = pd.read_table('attr.txt', sep=\":\", usecols=all, names = ['attr', 'range'])\n",
    "dfX_pred = pd.read_table('sample-test.txt', sep=\"\\s+\", usecols=all, names = list(attr['attr'])[:-1])\n",
    "dfX_pred = dfX_pred.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate prediction data by type\n",
    "X_pred_B = dfX_pred.loc[:, 'B1':'B3']\n",
    "X_pred_C = dfX_pred.loc[:,'C1':'C139']\n",
    "X_pred_CT = dfX_pred.loc[:,'CT1':'CT26']\n",
    "X_pred_H = dfX_pred.loc[:,'CH1':'CH4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"25_fet.pkl\", 'rb') as file:\n",
    "    attr_lst = pickle.load(file)\n",
    "list(attr_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive PCA\n",
    "X_pred = dfX_pred[attr_lst].to_numpy()\n",
    "pca = PCA(svd_solver='full', n_components='mle')\n",
    "X_pred = pca.fit_transform(X_pred)\n",
    "X_pred_new=np.concatenate((X_pred_B, X_pred), axis=1)\n",
    "X_pred_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous PCA\n",
    "# Reseperate C data to perform PCA\n",
    "non_C_ind = np.logical_or((np.core.defchararray.find(attr_lst,\"CT\")!=-1),(np.core.defchararray.find(attr_lst,\"CH\")!=-1))\n",
    "C_label = attr_lst[np.logical_not(non_C_ind)]\n",
    "print(C_label)\n",
    "X_pred = dfX_pred[C_label].to_numpy()\n",
    "print('Perform PCA on continuous features to filter noises')\n",
    "pca = PCA(svd_solver='full', n_components=no_component)\n",
    "X_pred = pca.fit_transform(X_pred)\n",
    "# Concat X_B with X to get the final attribute array\n",
    "X_pred_new=np.concatenate((X_pred_B, dfX_pred[attr_lst[non_C_ind]], X_pred), axis=1)\n",
    "X_pred_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model then predict and save prediction\n",
    "with open(\"91_model.pkl\", 'rb') as file:\n",
    "    search = pickle.load(file)\n",
    "y_pred = search.predict(X_pred_new)\n",
    "np.savetxt(fname=\"prediction.txt\", X=y_pred, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
